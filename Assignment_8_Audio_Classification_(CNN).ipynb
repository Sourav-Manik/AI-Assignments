{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14elDjze0VJznkL4A1zzQ45iCFjn8Todl",
      "authorship_tag": "ABX9TyOxt/p/D0D933aTPLrFNlHF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourav-Manik/AI-Assignments/blob/main/Assignment_8_Audio_Classification_(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio classification using CNN"
      ],
      "metadata": {
        "id": "NAIK7k__R_q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItuYTSvqzRCM"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "import csv \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Activation, Dense, Dropout, Conv2D, Flatten, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling1D, AveragePooling2D, Input, Add\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now convert the audio data files into PNG format images or basically extracting the Spectrogram for every Audio. (Below code is useful for CNN as we are converting data into image file)"
      ],
      "metadata": {
        "id": "ZPfQAfic6UYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = plt.get_cmap('inferno')\n",
        "plt.figure(figsize=(8,8))\n",
        "# Audio_Labels = 'fold1 fold2'.split()\n",
        "\n",
        "Audio_Labels = ['air conditioner' ,'car horn' ,'children playing', 'dog bark', 'drilling', 'engine idling', 'gunshot', 'jackhammer','siren','street music']\n",
        "\n",
        "# Audio_Labels.split()\n",
        "\n",
        "for g in Audio_Labels:\n",
        "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)\n",
        "    for filename in os.listdir(f'/content/drive/MyDrive/AI/Audio_Classification_Dataset/Audio_Labels/{g}'):\n",
        "        songname = f'/content/drive/MyDrive/AI/Audio_Classification_Dataset/Audio_Labels/{g}/{filename}'\n",
        "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
        "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
        "        plt.axis('off');\n",
        "        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
        "        plt.clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KJD7q_K_0gMN",
        "outputId": "9612a823-f34a-43bf-8f7c-4bdb90841b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split_folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-6AvsNnSIB2",
        "outputId": "d37f0565-b74f-4212-8ef3-f378cd956581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split_folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import splitfolders"
      ],
      "metadata": {
        "id": "kS-5l1CBSc-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "splitfolders.ratio('./img_data/', output=\"./data\", seed=1337, ratio=(.8, .2)) # default values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkuiX92hSIFN",
        "outputId": "5edb707c-39dc-4f33-ee48-51f7c6fa94b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 240 files [00:00, 4154.27 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "        rescale=1./255, # rescale all pixel values from 0-255, so aftre this step all our pixel values are in range (0,1)\n",
        "        shear_range=0.2, #to apply some random tranfromations\n",
        "        zoom_range=0.2, #to apply zoom\n",
        "        horizontal_flip=True) # image will be flipper horiztest_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "Xfz-ZlQXSIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = datagen.flow_from_directory(\n",
        "        './data/train',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle = False)\n",
        "\n",
        "test_set = datagen.flow_from_directory(\n",
        "        './data/val',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle = False )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZFhf7uZSIK0",
        "outputId": "5e187387-04d8-48fc-86e1-00689c6db1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 190 images belonging to 10 classes.\n",
            "Found 50 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_Model(): \n",
        "  global model \n",
        "  model = Sequential()\n",
        "  input_shape=(64, 64, 3)#1st hidden layer\n",
        "  model.add(Conv2D(32, (3, 3), strides=(2, 2), input_shape=input_shape))\n",
        "  model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
        "  model.add(Activation('relu'))#2nd hidden layer\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "  model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
        "  model.add(Activation('relu'))#3rd hidden layer\n",
        "  model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "  model.add(AveragePooling2D((2, 2), strides=(2,2)))\n",
        "  model.add(Activation('relu'))#Flatten\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(rate=0.5))#Add fully connected layer.\n",
        "  model.add(Dense(64))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(rate=0.5))#Output layer\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  epochs = 50\n",
        "  batch_size = 4\n",
        "  learning_rate = 0.01\n",
        "  decay_rate = learning_rate / epochs\n",
        "  momentum = 0.9\n",
        "  sgd = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "  model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "  return CNN_Model\n"
      ],
      "metadata": {
        "id": "ver54gGlSIOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLV3H7_kvd0E",
        "outputId": "00545660-facd-4c88-e090-ad6cad32d632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 31, 31, 32)        896       \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 15, 15, 32)       0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 15, 15, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 7, 7, 64)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 3, 3, 64)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 3, 3, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                36928     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,898\n",
            "Trainable params: 93,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "        training_set,\n",
        "        steps_per_epoch= 6//4, #len(train_set)//batch_size\n",
        "        epochs=50,\n",
        "        validation_data=test_set,\n",
        "        validation_steps=200)"
      ],
      "metadata": {
        "id": "b7cv_uR6UKJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d1868e-1248-49a9-fc63-965f57d683a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3071 - accuracy: 0.0938"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 2s 2s/step - loss: 2.3071 - accuracy: 0.0938 - val_loss: 2.2988 - val_accuracy: 0.1600\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 2.2512 - accuracy: 0.1562\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 2.3402 - accuracy: 0.1562\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 358ms/step - loss: 2.3239 - accuracy: 0.0667\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 2.3764 - accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 343ms/step - loss: 2.3125 - accuracy: 0.0312\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 2.2551 - accuracy: 0.2500\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 2.3012 - accuracy: 0.0312\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 2.2502 - accuracy: 0.2500\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 2.2992 - accuracy: 0.0333\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 2.3221 - accuracy: 0.1000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 2.2683 - accuracy: 0.1562\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 2.3660 - accuracy: 0.0312\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 2.2744 - accuracy: 0.1333\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 2.3305 - accuracy: 0.0625\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 2.2480 - accuracy: 0.3000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 2.3065 - accuracy: 0.0312\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 2.2457 - accuracy: 0.3333\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 2.3118 - accuracy: 0.0938\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 2.2752 - accuracy: 0.0312\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 2.2870 - accuracy: 0.0938\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 2.2507 - accuracy: 0.2812\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 2.3608 - accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 2.2311 - accuracy: 0.2333\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 2.3626 - accuracy: 0.0312\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 2.3229 - accuracy: 0.0312\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 2.3761 - accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 2.2786 - accuracy: 0.0625\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 2.3375 - accuracy: 0.0625\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 349ms/step - loss: 2.3787 - accuracy: 0.0312\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 2.2455 - accuracy: 0.2333\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 2.2986 - accuracy: 0.0625\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 2.3644 - accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 368ms/step - loss: 2.3509 - accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 350ms/step - loss: 2.3265 - accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 2.3176 - accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 357ms/step - loss: 2.2993 - accuracy: 0.0312\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 372ms/step - loss: 2.3301 - accuracy: 0.1250\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 374ms/step - loss: 2.2867 - accuracy: 0.1562\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 2.2669 - accuracy: 0.2500\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 369ms/step - loss: 2.3181 - accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 347ms/step - loss: 2.2620 - accuracy: 0.3125\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 2.3254 - accuracy: 0.0312\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 346ms/step - loss: 2.2689 - accuracy: 0.0938\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 2.3066 - accuracy: 0.0625\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 2.3674 - accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 370ms/step - loss: 2.2950 - accuracy: 0.0938\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 366ms/step - loss: 2.3121 - accuracy: 0.0312\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 373ms/step - loss: 2.2775 - accuracy: 0.2812\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 375ms/step - loss: 2.2833 - accuracy: 0.1250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8aed3f9820>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "model.evaluate_generator(generator=test_set, steps=50)#OUTPUT"
      ],
      "metadata": {
        "id": "w-YzMFY-UKM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b40ea72-39e1-4144-f445-39723cbb4ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.299072504043579, 0.20000000298023224]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "End"
      ],
      "metadata": {
        "id": "_tjgXLd_1f50"
      }
    }
  ]
}